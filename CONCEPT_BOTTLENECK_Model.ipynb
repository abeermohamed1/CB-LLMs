{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abeermohamed1/CB-LLMs/blob/main/CONCEPT_BOTTLENECK_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNo3Czo8tO2j",
        "outputId": "23270c02-2923-4203-cbbb-dbb0cfd5d59c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CB-LLMs'...\n",
            "remote: Enumerating objects: 179, done.\u001b[K\n",
            "remote: Counting objects: 100% (107/107), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 179 (delta 74), reused 35 (delta 35), pack-reused 72 (from 1)\u001b[K\n",
            "Receiving objects: 100% (179/179), 952.32 KiB | 5.09 MiB/s, done.\n",
            "Resolving deltas: 100% (85/85), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/abeermohamed1/CB-LLMs.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aec7265",
        "outputId": "6f537e6a-8eb3-4e2b-8dac-d652574a5b79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CB-LLMs/classification\n"
          ]
        }
      ],
      "source": [
        "%cd CB-LLMs/classification/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b26e1501",
        "outputId": "1ff9b2f9-656c-4e5c-8076-e1c4f76e460a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==2.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.2.0)\n",
            "Requirement already satisfied: transformers==4.44.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (4.44.0)\n",
            "Requirement already satisfied: datasets==3.0.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: peft==0.13.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (0.13.2)\n",
            "Requirement already satisfied: glm_saga==0.1.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (0.1.2)\n",
            "Requirement already satisfied: evaluate==0.4.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (0.4.0)\n",
            "Requirement already satisfied: torchvision==0.17.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (0.17.0)\n",
            "Requirement already satisfied: scikit-learn==1.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (1.3.0)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 1)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 1)) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 1)) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 1)) (2024.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 1)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 1)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 1)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 1)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 1)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 1)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 1)) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 1)) (2.2.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.0->-r requirements.txt (line 2)) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.0->-r requirements.txt (line 2)) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.0->-r requirements.txt (line 2)) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.0->-r requirements.txt (line 2)) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.0->-r requirements.txt (line 2)) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.0->-r requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.0->-r requirements.txt (line 2)) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.0->-r requirements.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.0.2->-r requirements.txt (line 3)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.0.2->-r requirements.txt (line 3)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==3.0.2->-r requirements.txt (line 3)) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==3.0.2->-r requirements.txt (line 3)) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets==3.0.2->-r requirements.txt (line 3)) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==3.0.2->-r requirements.txt (line 3)) (3.13.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft==0.13.2->-r requirements.txt (line 4)) (5.9.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.13.2->-r requirements.txt (line 4)) (1.12.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.12/dist-packages (from evaluate==0.4.0->-r requirements.txt (line 6)) (0.18.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.17.0->-r requirements.txt (line 7)) (11.3.0)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.3.0->-r requirements.txt (line 8)) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.3.0->-r requirements.txt (line 8)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.3.0->-r requirements.txt (line 8)) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0->-r requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.0.2->-r requirements.txt (line 3)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.0.2->-r requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.0.2->-r requirements.txt (line 3)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.0.2->-r requirements.txt (line 3)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.0.2->-r requirements.txt (line 3)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.0.2->-r requirements.txt (line 3)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.0.2->-r requirements.txt (line 3)) (1.22.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.0->-r requirements.txt (line 2)) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.44.0->-r requirements.txt (line 2)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.44.0->-r requirements.txt (line 2)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.44.0->-r requirements.txt (line 2)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.44.0->-r requirements.txt (line 2)) (2025.11.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.2.0->-r requirements.txt (line 1)) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.0.2->-r requirements.txt (line 3)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.0.2->-r requirements.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.0.2->-r requirements.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.2.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.0.2->-r requirements.txt (line 3)) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a94adb6",
        "outputId": "f9f603f3-402a-4663-9c53-827072427725"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated git hooks.\n",
            "Git LFS initialized.\n",
            "Cloning into 'temp_repo'...\n",
            "remote: Enumerating objects: 138, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 138 (delta 0), reused 0 (delta 0), pack-reused 135 (from 1)\u001b[K\n",
            "Receiving objects: 100% (138/138), 20.01 KiB | 10.01 MiB/s, done.\n",
            "\n",
            "Exiting because of \"interrupt\" signal.\n",
            "warning: Clone succeeded, but checkout failed.\n",
            "You can inspect what was checked out with 'git status'\n",
            "and retry with 'git restore --source=HEAD :/'\n",
            "\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/cesun/cbllm-classification temp_repo\n",
        "!mv temp_repo/mpnet_acs .\n",
        "!rm -rf temp_repo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python get_concept_labels.py --dataset yelp_polarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FfEyqN_pX2U",
        "outputId": "59b463dc-80a4-408f-f03e-330b7d92b024"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "\r0it [00:00, ?it/s]\r0it [00:00, ?it/s]\n",
            "loading data...\n",
            "README.md: 8.93kB [00:00, 1.71MB/s]\n",
            "plain_text/train-00000-of-00001.parquet: 100% 256M/256M [00:03<00:00, 66.5MB/s]\n",
            "plain_text/test-00000-of-00001.parquet: 100% 17.7M/17.7M [00:00<00:00, 19.1MB/s]\n",
            "Generating train split: 100% 560000/560000 [00:01<00:00, 358147.79 examples/s]\n",
            "Generating test split: 100% 38000/38000 [00:00<00:00, 368624.27 examples/s]\n",
            "training data len:  560000\n",
            "concept len:  248\n",
            "tokenizing and preparing mpnet\n",
            "tokenizer_config.json: 100% 363/363 [00:00<00:00, 2.12MB/s]\n",
            "vocab.txt: 232kB [00:00, 10.2MB/s]\n",
            "tokenizer.json: 466kB [00:00, 10.3MB/s]\n",
            "special_tokens_map.json: 100% 239/239 [00:00<00:00, 1.02MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "config.json: 100% 571/571 [00:00<00:00, 3.95MB/s]\n",
            "model.safetensors: 100% 438M/438M [00:02<00:00, 210MB/s]\n",
            "Map:   0% 0/560000 [00:00<?, ? examples/s]^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_CBL.py --automatic_concept_correction --dataset yelp_polarity"
      ],
      "metadata": {
        "id": "YdUcR0eNphN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python train_FL.py --cbl_path mpnet_acs/SetFit_sst2/roberta_cbm/cbl_ac"
      ],
      "metadata": {
        "id": "XFevWjR4p2iS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "b93f36e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3482d0a-6de3-4e03-9e4a-7582c8f618d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "torch==2.2.0\n",
        "transformers==4.44.0\n",
        "datasets==3.0.2\n",
        "peft==0.13.2\n",
        "glm_saga==0.1.2\n",
        "evaluate==0.4.0\n",
        "torchvision==0.17.0\n",
        "scikit-learn==1.3.0\n",
        "numpy==1.26.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1e07db41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abda4c83-c65d-463d-9cc5-350dd7f17ca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-09 19:47:15.002234: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765309635.218140    1976 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765309635.281356    1976 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765309635.743504    1976 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765309635.743544    1976 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765309635.743549    1976 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765309635.743553    1976 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-09 19:47:15.784277: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "loading data...\n",
            "README.md: 100% 378/378 [00:00<00:00, 2.56MB/s]\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n",
            "train.jsonl: 100% 1.07M/1.07M [00:01<00:00, 919kB/s]\n",
            "dev.jsonl: 100% 136k/136k [00:00<00:00, 326kB/s]\n",
            "test.jsonl: 100% 281k/281k [00:00<00:00, 710kB/s]\n",
            "Generating train split: 100% 6920/6920 [00:00<00:00, 303173.14 examples/s]\n",
            "Generating validation split: 100% 872/872 [00:00<00:00, 378616.26 examples/s]\n",
            "Generating test split: 100% 1821/1821 [00:00<00:00, 597966.62 examples/s]\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n",
            "training data len:  6920\n",
            "val data len:  872\n",
            "tokenizing...\n",
            "tokenizer_config.json: 100% 25.0/25.0 [00:00<00:00, 218kB/s]\n",
            "vocab.json: 100% 899k/899k [00:01<00:00, 533kB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 1.10MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 6.07MB/s]\n",
            "config.json: 100% 481/481 [00:00<00:00, 4.27MB/s]\n",
            "Map:   0% 0/6920 [00:00<?, ? examples/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/CB-LLMs/classification/finetune_black_box.py\", line 62, in <module>\n",
            "    encoded_train_dataset = train_dataset.map(\n",
            "                            ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\", line 560, in wrapper\n",
            "    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\", line 3318, in map\n",
            "    for rank, done, content in Dataset._map_single(**unprocessed_kwargs):\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\", line 3674, in _map_single\n",
            "    for i, batch in iter_outputs(shard_iterable):\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\", line 3624, in iter_outputs\n",
            "    yield i, apply_function(example, i, offset=offset)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\", line 3547, in apply_function\n",
            "    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/CB-LLMs/classification/finetune_black_box.py\", line 63, in <lambda>\n",
            "    lambda e: tokenizer(e[CFG.example_name[args.dataset]], padding=True, truncation=True, max_length=args.max_length), batched=True,\n",
            "                          ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "KeyError: 'SetFit/sst2'\n"
          ]
        }
      ],
      "source": [
        "!python finetune_black_box.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b636a1b1",
        "outputId": "595786b0-ec8f-4e94-eadf-505a928842e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-09 19:47:48.906107: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765309668.918780    2208 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765309668.922681    2208 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765309668.936699    2208 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765309668.936726    2208 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765309668.936730    2208 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765309668.936733    2208 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-09 19:47:48.942266: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "loading data...\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n",
            "test data len:  1821\n",
            "tokenizing...\n",
            "Map:   0% 0/1821 [00:00<?, ? examples/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/CB-LLMs/classification/print_concept_contributions.py\", line 64, in <module>\n",
            "    encoded_test_dataset = test_dataset.map(\n",
            "                           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\", line 560, in wrapper\n",
            "    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\", line 3318, in map\n",
            "    for rank, done, content in Dataset._map_single(**unprocessed_kwargs):\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\", line 3674, in _map_single\n",
            "    for i, batch in iter_outputs(shard_iterable):\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\", line 3624, in iter_outputs\n",
            "    yield i, apply_function(example, i, offset=offset)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\", line 3547, in apply_function\n",
            "    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/CB-LLMs/classification/print_concept_contributions.py\", line 65, in <lambda>\n",
            "    lambda e: tokenizer(e[CFG.example_name[dataset]], padding=True, truncation=True,\n",
            "                          ~~~~~~~~~~~~~~~~^^^^^^^^^\n",
            "KeyError: 'SetFit/sst2'\n"
          ]
        }
      ],
      "source": [
        "!python print_concept_contributions.py --cbl_path mpnet_acs/SetFit_sst2/roberta_cbm/cbl_acc.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a182125",
        "outputId": "782e27aa-8ffb-4b27-c0f5-34e70f979a2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-09 19:48:04.733960: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765309684.746518    2286 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765309684.750382    2286 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765309684.761073    2286 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765309684.761096    2286 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765309684.761101    2286 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765309684.761104    2286 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-09 19:48:04.764226: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/CB-LLMs/classification/test_CBLLM.py\", line 8, in <module>\n",
            "    import evaluate\n",
            "ModuleNotFoundError: No module named 'evaluate'\n"
          ]
        }
      ],
      "source": [
        "!python test_CBLLM.py --cbl_path mpnet_acs/SetFit_sst2/roberta_cbm/cbl_acc.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58ff54bb",
        "outputId": "955d6e24-89c2-4602-8065-02dee77e58db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-09 19:48:15.918962: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765309695.931245    2339 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765309695.935177    2339 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765309695.945692    2339 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765309695.945711    2339 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765309695.945715    2339 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765309695.945718    2339 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-09 19:48:15.948800: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "loading data...\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n",
            "test data len:  1821\n",
            "tokenizing...\n",
            "Map:   0% 0/1821 [00:00<?, ? examples/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/CB-LLMs/classification/print_concept_activations.py\", line 62, in <module>\n",
            "    encoded_test_dataset = test_dataset.map(lambda e: tokenizer(e[CFG.example_name[dataset]], padding=True, truncation=True, max_length=args.max_length), batched=True, batch_size=len(test_dataset))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\", line 560, in wrapper\n",
            "    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\", line 3318, in map\n",
            "    for rank, done, content in Dataset._map_single(**unprocessed_kwargs):\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\", line 3674, in _map_single\n",
            "    for i, batch in iter_outputs(shard_iterable):\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\", line 3624, in iter_outputs\n",
            "    yield i, apply_function(example, i, offset=offset)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\", line 3547, in apply_function\n",
            "    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/CB-LLMs/classification/print_concept_activations.py\", line 62, in <lambda>\n",
            "    encoded_test_dataset = test_dataset.map(lambda e: tokenizer(e[CFG.example_name[dataset]], padding=True, truncation=True, max_length=args.max_length), batched=True, batch_size=len(test_dataset))\n",
            "                                                                  ~~~~~~~~~~~~~~~~^^^^^^^^^\n",
            "KeyError: 'SetFit/sst2'\n"
          ]
        }
      ],
      "source": [
        "!python print_concept_activations.py --cbl_path mpnet_acs/SetFit_sst2/roberta_cbm/cbl_acc.pt"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpuPYTYHbro08ctE48Ycn7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}